---
created: 2023-11-12
title: N-Beats (2019)
layout: post
tags: [n-beats, time-series]
category: ML
image:
  path: https://i.imgur.com/GwpmTTQ.png
  alt: 
---

📄 Oreshkin, Boris N., et al. "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting." _arXiv preprint arXiv:1905.10437_ (2019).

## 들어가며

2018년 시계열 모델 경진대회인 M4 Competition이 열렸었는데요. 해당 대회에서 재밌게 여길만한 점은 바로 순수 ML 모델들의 성적입니다. 총 60개의 팀에서 여섯 개의 팀만이 순수 ML 모델을 제출하였고, 해당 팀들의 순위 중 가장 높은 성적은 23위였습니다. 참고로 해당 대회에서 우승을 차지한 모델은 통계적인 방법론과 ML 방법론을 섞은 [ES-RNN (Exponential Smoothing Recurrent Neural Network)](https://arxiv.org/abs/1907.03329) 이었습니다.

본 논문은 이런 상황에서 해당 대회에 제출된 모델보다 성능이 좋은 순수 ML 기반의 모델인 **N-Beats**를 제안합니다. 이 논문에서 제시하고 있는 모델의 장점은 다음과 같습니다.

- **Deep Neural Architecture**
	- 기존 여러 데이터(M3, M4, `TOURISM` 등)에 대해 통계적 접근법보다 좋은 성능을 보이는 순수 DL 기반의 모델입니다.
- **해석 가능한 시계열 딥러닝 모델**
	- **계절성-추세 수준**의 접근 방식과 같은 전통적인 분해 기법과 비슷한 방식으로 모델을 해석 가능한 아키텍처로 설계하는 것이 가능합니다.

본 모델에 대해서 알아보기 전에 몇 가지 표기법(notation)을 짚고 넘어가겠습니다. <u>이산적 시간에 대한 단변량 예측 문제</u>에 대해서 다음을 정의합니다.
- $H$ 길이만큼의 예측 범위
	- $\mathbf{y} = [y_{T+1}, \cdots, y_{T+H}] \in \mathbb{R}^H$
- $T$ 길이만큼의 과거 이력
	- $[y_1, \cdots, y_T] \in \mathbb{R}^T$
- 길이 $t \leq T$의 **lookback window**
	- $\mathbf{x} = [y_{T-t+1}, \cdots, y_T] \in \mathbb{R}^t$
- $\mathbf{y}$를 예측한 값 $\hat{\mathbf{y}}$

## N-Beats

N-Beats의 아키텍처를 설계할 때 다음의 포인트를 중요하게 여겼다고 합니다.

1. 기본 아키텍처는 단순하고 일반적이되 높은 표현력을 갖고 있어야 함
2. 시계열에 특화된 피처 엔지니어링이나 입력값 스케일링 등에 의존하지 않는 아키텍처여야 함
3. 사람이 결과를 해석할 수 있도록 확장 가능한 아키텍처여야 함

이런 포인트를 포함하고 있는 N-Beats의 아키텍처는 아래 다이어그램과 같습니다.

![](https://i.imgur.com/GwpmTTQ.png)
_N-Beats architecture_

### Basic Block

![](https://i.imgur.com/gGBdXdF.png)
_The detailed architecture of a basic block._

기본 블록의 형태는 위 이미지와 같습니다. 이런 기본 블록을 여러 개 쌓아 하나의 스택을 만드는데, 일반적인 설명을 위해 $\ell$ 번째 블록에 대해 다루겠습니다. 

$\ell$ 번째 블록에 대하여 해당 블록의 입력 벡터인 $\mathbf{x}\_\ell$ 이 있습니다. 만약 $\ell = 1$ 이라면 맨 처음 블록이므로 $\mathbf{x}\_\ell$ 은 모델의 입력 벡터와 같아집니다. 그리고 우리가 예측할 범위의 길이를 $H$ 라고 하면 최초 블록의 입력값이 되는 벡터의 길이는 보통 $2H$ 에서 $7H$ 로 설정합니다. 즉 예측하는 타임스탬프 길이의 두 배에서 일곱 배의 데이터를 입력값으로 사용합니다.

하지만 다른 경우에는 모두 이전 블록의 **residual output**을 입력으로 받습니다. 그리고 두 개의 아웃풋 벡터 $\hat{\mathbf{x}}\_\ell$, $\hat{\mathbf{y}}\_\ell$ 가 있습니다. 각각은 다음과 같습니다.

- $\hat{\mathbf{x}}\_\ell$ : Backcast 예측
	- 입력 벡터에 대한 예측입니다.
- $\hat{\mathbf{y}}\_\ell$ : Forecast 예측
	- 실제로 예측할 범위에 대한 예측입니다.

이런 설정 아래 기본 블록은 네 개의 FC 레이어를 거쳐서 두 개의 분기로 나눠지는데 각 분기에서 backcast와 forecast에 대한 예측 계수 $\theta\_\ell^b$ 와 $\theta\_\ell^f$를 얻게 됩니다. 여기까지를 수식으로 나타내면 다음과 같습니다.

- $\mathbf{h}\_{\ell, 1} = \text{FC}\_{\ell, 1}(\mathbf{x}\_\ell)$
- $\mathbf{h}\_{\ell, 2} = \text{FC}\_{\ell, 2}(\mathbf{h}\_{\ell, 1})$
- $\mathbf{h}\_{\ell, 3} = \text{FC}\_{\ell, 3}(\mathbf{h}\_{\ell, 2})$
- $\mathbf{h}\_{\ell, 4} = \text{FC}\_{\ell, 4}(\mathbf{h}\_{\ell, 3})$
- $\theta_\ell^b = \text{Linear}^b_\ell (\mathbf{h}_{\ell, 4})$
- $\theta_\ell^f = \text{Linear}^f_\ell (\mathbf{h}_{\ell, 4})$

여기서 FC 는 fully connected layer와 ReLU로 구성합니다.

$$\mathbf{h}_{\ell, 1} = \text{ReLU}(\mathbf{W}_{\ell, 1} \mathbf{x}_\ell + \mathbf{b}_{\ell, 1})$$

마지막으로 기저 레이어(basis layer) $g_\ell^b$와 $g_\ell^f$를 거쳐 다음을 계산합니다.

$$\hat{\mathbf{y}_\ell} = g_\ell^f(\theta_\ell^f) = \sum_{i=1}^{\text{dim}(\theta_\ell^f)} \theta_{\ell, i}^f \mathbf{v}_i^f, \qquad \hat{\mathbf{x}_\ell} = g_\ell^b(\theta_\ell^b) = \sum_{i=1}^{\text{dim}(\theta_\ell^b)} \theta_{\ell, i}^b \mathbf{v}_i^b$$

이때 기저 레이어는 학습 가능한 파라미터로 설정할 수도 있고 특정 함수 형태로 설정할 수도 있습니다.

### Doubly Residual Stacking

![](https://i.imgur.com/VFwiUCD.png)
_Doubly residual stacking_

일반적인 residual connection은 입력값을 몇 개의 레이어를 건너 뛰어 **더하는** 방식을 사용합니다. 이렇게 하면 더 깊은 구조를 잘 학습하는 이점이 있습니다. 하지만 해석 가능한 아키텍처를 구성하는 경우에는 도움이 되지 못합니다. 그래서 저자는 기존 값을 더하는 대신 **빼는** 방식을 채용했습니다. Backcast에서 블록의 입력 벡터와 현재 블록의 backcast를 뺀 residual을 다음 블록으로 넘겨주는 방식입니다.

$$\mathbf{x}_\ell = \mathbf{x}_{\ell-1} - \hat{\mathbf{x}}_{\ell -1}$$

Forecast는 residual connection 없이 매 블록의 forecast를 더합니다.

$$\hat{\mathbf{y}} = \sum_\ell \hat{\mathbf{y}}_\ell$$

이 구조를 통해 얻는 효과는 다음과 같습니다.

1. 이전 블록이 입력 벡터의 일부 시그널 $\hat{\mathbf{x}}\_{\ell-1}$ 을 제거하여 블록의 예측 작업을 쉽게 만들어줍니다.
2. Backcast의 residual connection 구조로 인해 그라디언트가 더 잘 흘러 역전파를 용이하게 합니다.
3. Forecast의 summation connection 구조는 **계층적 분해(hierarchical decompostion)** 를 가능하게 합니다.
	- $g_\ell^b$와 $g_\ell^f$로 인해 강제되는 의도적인 구조는 forecast의 계층적 분해가 모델의 해석을 가능하게 하는 중요한 의미를 가집니다.

![](https://i.imgur.com/xKdWRJz.png)
위 내용까지를 하나의 스택으로 구성해서 stack residual은 다음 스택으로, 각 스택의 stack forecast는 모두 합해서 global forecast를 얻는 방식입니다. 이때 학습은 MSE를 손실 함수로 해서 진행합니다.

### Interpretability

N-Beats는 $g_\ell^b$와 $g_\ell^f$를 설정하는 방법에 따라 두 개의 아키텍처로 나뉩니다. 지금까지 다룬 일반적인 아키텍처(Generic architecture)는 시계열에 특화된 지식에 의존하지 않습니다. 하지만 이제부터 설명할 **해석 가능한 아키텍처(interpretable architecture)** 는 해석력을 위해서 유도 편향(inductive bias)를 추가했습니다. 이때 시계열에 대한 정보가 들어가죠.

일반적인 아키텍처는 $g_\ell^b$와 $g_\ell^f$를 이전 레이어 아웃풋의 linear projection으로 설정합니다.

$$\hat{\mathbf{y}}_\ell = \mathbf{V}_\ell^f \theta_\ell^f + \mathbf{b}_\ell^f \qquad \hat{\mathbf{x}}_\ell = \mathbf{V}_\ell^b \theta_\ell^b + \mathbf{b}_\ell^b$$

이때 $\mathbf{V}\_\ell^f$ 는 $H \times \dim(\theta\_\ell^f)$ 의 차원을 가집니다.

해석 가능한 아키텍처는 $g_\ell^b$와 $g_\ell^f$ 를 어떻게 설정하느냐에 따라 **추세 모델(trend model)** 과 **계절성 모델(seasonality model)** 로 나뉩니다.

#### Trend model

![](https://i.imgur.com/sJpvAYS.png)
추세의 일반적인 특성이라고 하면 단조증가 또는 단조감소하는 형태를 갖거나 천천히 변화하는 형태를 갖는다는 점입니다. 이런 특성을 나타내기 위해서 저자는  작은 차수의 다항함수 형태를 차용했습니다.

$$\hat{\mathbf{y}}_{s, \ell} = \sum^p_{i=0} \theta_{s, \ell, i}^f t^i \quad \text{where } \mathbf{t} = [0, 1, 2, \cdots, H-2, H-1]^T/H$$

행렬식으로 나타내면 다음과 같습니다.

$$\hat{\mathbf{y}}_{s, \ell}^{tr} = \mathbf{T}\theta_{s, \ell}^f \quad \text{where } \mathbf{T} = [\mathbf{1}, \mathbf{t}, \cdots, \mathbf{t}^p]$$

이해를 돕기 위해 위 그림을 참고하자면 $g^b$ 와 $g^f$ 를 특정 행렬 형태로 설정합니다. 각 행은 backcast 또는 forecast의 time step을 나타내고 각 열은 다항함수의 차수만큼으로 구성되어 있습니다. $p$를 적당하게 작게 설정하면 $\hat{\mathbf{y}}_{s, \ell}^{tr}$은 추세를 따라가게 됩니다.

#### Seasonality Model

![](https://i.imgur.com/1kKwQNo.png)

계절성은 규칙적이고 주기적이며 반복적인 변동이 있습니다. 이런 특성을 나타내기 위해 주기 함수를 차용했는데요. 가장 적절한 선택은 여러모로 푸리에 급수입니다.

$$\hat{\mathbf{y}}_{s, \ell} = \sum^{\lfloor H/2 - 1 \rfloor}_{i=0} \theta_{s, \ell, i}^f \cos(2\pi it) + \theta^f_{s, \ell, i+\lfloor H/2 \rfloor} \sin(2\pi it)$$

행렬식으로 나타내면 다음과 같습니다.

$$\begin{aligned} 
&\hat{\mathbf{y}}_{s, \ell}^{seas} = \mathbf{S}_{s, \ell}^f \\ & \quad \text{where } \mathbf{S} = [\mathbf{1}, \cos(2\pi\mathbf{t}), \cdots, \cos(2\pi\lfloor H/2-1 \rfloor \mathbf{t}), \sin(2\pi\mathbf{t}), \cdots, \sin(2\pi \lfloor H/2 -1 \rfloor \mathbf{t})]
\end{aligned}$$

마지막으로 추세 모델과 계절성 모델을 아래 그림처럼 붙여주면 됩니다. 각 추세 블록과 계절성 블록으로 스택을 구성하며, 각 블록은 일반적인 아키텍처와 동일하게 residual connection을 활용합니다.

![](https://i.imgur.com/xv94BLG.png)
## Implementation

N-Beats를 구현한 코드는 [다음 저장소](https://github.com/philipperemy/n-beats)에서 확인하실 수 있습니다. 여러 데이터셋에 대한 실험 코드도 포함하고 있으며, 논문과 동일한 결과를 얻기 위한 가이드 역시 수록되어 있습니다. 단, 일반적인 아키텍처는 구현하기 쉽지만 해석 가능한 아키텍처는 구현도 까다롭고 해석도 쉽지 않습니다.